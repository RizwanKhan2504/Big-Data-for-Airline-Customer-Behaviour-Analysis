# -*- coding: utf-8 -*-
"""Copy of Airline Review1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dhYMBSn7U6hlUT7iRFLkwo1XT-bzv7nn
"""

! ls

from google.colab import drive
drive.mount('/content/drive')

"""# Data Preprocessing

**<font color=”blue”> 1. Import Libraries </font>**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

df = pd.read_csv("drive/MyDrive/big_data/Airline_Review.csv")

df.describe(include="all")

df.shape

df.size

df.isna().sum()

df.info()

df.nunique()

df.duplicated()

columns_to_check = ['Passanger_Name', 'Flying_month', 'Route', 'Rating', 'Review_title','Review_content','Traveller_type', 'Class' ]

# Finding and displaying duplicate rows based on specified columns
duplicate_rows = df[df.duplicated(subset=columns_to_check, keep=False)]

# Displaying only the duplicate rows
print(duplicate_rows)

duplicate_rows.to_csv('duplicate_rows.csv', index=False)

df.duplicated().sum()

df.describe(include='all')

df.count()

df

"""# **Removing Duplicates values**"""

columns_to_check = ['Passanger_Name', 'Flying_month', 'Route', 'Rating', 'Verified', 'Review_title', 'Review_content', 'Traveller_type', 'Class' ]

# Remove duplicate rows based on the specified columns
df_no_duplicates = df.drop_duplicates(subset=columns_to_check, keep='first')

# df_no_duplicates will contain the DataFrame with duplicate rows removed
df_no_duplicates

df.count()

df=df_no_duplicates

df.dtypes['Flying_month']

import missingno as msno
msno.matrix(df)
plt.show()
msno.bar(df)
plt.show()
msno.heatmap(df)
plt.show()
msno.dendrogram(df)
plt.show()



"""# Cleaning data for flying month column"""

# Define a regular expression pattern for 'mmm-yy' format
date_pattern = r'^[A-Z][a-z]{2}-\d{2}$'

# Filter rows that do not match the pattern
non_matching_dates = df[~df['Flying_month'].str.match(date_pattern, na=False)]

# Create a new column and copy non-matching values
df['Non_Date_Column'] = None
df.loc[non_matching_dates.index, 'Non_Date_Column'] = non_matching_dates['Flying_month']

df

# Changing the name of the column from Non_Date_Column to New_route
df.rename(columns={'Non_Date_Column':'Route_copy'}, inplace=True)

df

# Replacing all the values of date with np.nan from the Flying_month column
df.loc[~df['Flying_month'].str.match(date_pattern, na=False), 'Flying_month'] = np.nan

df

"""# Cleaning data for Route column"""

# Define a regular expression pattern for 'mmm-yy' format
date_pattern = r'^[A-Z][a-z]{2}-\d{2}$'

# Filter rows that do not match the pattern
non_matching_dates = df[df['Route'].str.match(date_pattern, na=True)]

# Create a new column and copy non-matching values
df['Flying_month_copy'] = None  # Initialize the new column with None values
df.loc[non_matching_dates.index, 'Flying_month_copy'] = non_matching_dates['Route']

df

df.to_csv(r'C:\Users\RRK\Desktop\Northumbria\ML Assesment\new1.csv')

df

df.loc[df['Route'].str.match(date_pattern, na=True), 'Route'] = np.nan

df.to_csv(r'C:\Users\RRK\Desktop\Northumbria\ML Assesment\new1.csv')

# Define a regular expression pattern to extract specific class names
class_pattern = r'(?i)(?:Economy Class|Premium Economy|Business Class|First Class)'

# Filter rows that do not match the pattern
matching_class = df[df['Route'].str.match(class_pattern, na=True)]

# Create a new column and copy non-matching values
df['Class_copy'] = None  # Initialize the new column with None values
df.loc[matching_class.index, 'Class_copy'] = matching_class['Route']

df

# Replace all the values of class with np.nan from the route column
df.loc[df['Route'].str.match(class_pattern, na=True), 'Route'] = np.nan

df

df.to_csv(r'C:\Users\RRK\Desktop\Northumbria\ML Assesment\new1.csv')

"""# Cleaning data for class column"""

# Copy the 'class' column to a new column named 'traveller_type_copy'
df['traveller_type_copy'] = df['Class'].copy()

df

# Checking the unique values in the traveller_type_copy
unique_values_column = df['traveller_type_copy'].unique()
print(unique_values_column)

# List of strings to be removed from 'traveller_type_copy'
strings_to_remove = ['Economy Class', 'Business Class','Business', 'Premium Economy',
                     'London to Malaga','First Class', 'LHR to ORD', 'Los Angeles to London to Paris to Rome ',
                     'no','yes']

# Remove multiple specific string values from 'traveller_type_copy'
for string in strings_to_remove:
    df['traveller_type_copy'] = df['traveller_type_copy'].replace(string, 'None')

df

# Checking the unique values in the traveller_type_copy column
unique_values_column = df['traveller_type_copy'].unique()
print(unique_values_column)

# Copy the 'class' column to a new column named 'traveller_type_copy'
df['Route_copy2'] = df['Class'].copy()

# Checking the unique values in the traveller_type_copy column
unique_values_column = df['Route_copy2'].unique()
print(unique_values_column)

# List of strings to be removed from 'Route_copy2' column
strings_to_remove = ['Economy Class', 'Business Class', 'Solo Leisure', 'Family Leisure','Business', 'Couple Leisure',
                     'Premium Economy', 'First Class', 'no', 'yes']

# Remove multiple specific string values from 'Route_copy2' column
for string in strings_to_remove:
    df['Route_copy2'] = df['Route_copy2'].replace(string, 'None')

# Checking the unique values in the traveller_type_copy column
unique_values_column = df['Route_copy2'].unique()
print(unique_values_column)

# Copy the 'class' column to a new column named 'Yes_No_copy'
df['Yes_No_copy'] = df['Class'].copy()

# List of strings to be removed from 'Yes_No_copy' column
strings_to_remove = ['Economy Class', 'Business Class', 'Solo Leisure', 'Family Leisure', 'Business', 'Couple Leisure',
                     'Premium Economy', 'London to Malaga','First Class', 'LHR to ORD', 'Los Angeles to London to Paris to Rome ']

# Remove multiple specific string values from 'Yes_No_copy' column
for string in strings_to_remove:
    df['Yes_No_copy'] = df['Yes_No_copy'].replace(string, 'None')

# Checking the unique values in the Yes_No_copy column
unique_values_column = df['Yes_No_copy'].unique()
print(unique_values_column)

# Checking the unique values in the Class column
unique_values_column = df['Class'].unique()
print(unique_values_column)

# List of strings to be removed from 'Class' column
strings_to_remove =['Solo Leisure', 'Family Leisure', 'Couple Leisure', 'London to Malaga', 'LHR to ORD',
                    'Los Angeles to London to Paris to Rome ', 'no', 'yes']

# Remove multiple specific string values from 'Class' column
for string in strings_to_remove:
    df['Class'] = df['Class'].replace(string, 'None')

# Checking the unique values in the Class column
unique_values_column = df['Class'].unique()
print(unique_values_column)

df

df.to_csv(r'C:\Users\RRK\Desktop\Northumbria\ML Assesment\new1.csv')

"""# Cleaning data for Traveller_type column"""

# Checking the unique values in the Traveller_type column
unique_values_column = df['Traveller_type'].unique()
print(unique_values_column)

# Create a new column 'Type of Flight' and initialize it with NaN
df['Type_of_flight'] = np.nan

# Condition to move specific values to Type_of_flight column from Traveller_type column
conditions = ~df['Traveller_type'].isin(['Solo Leisure', 'Family Leisure', 'Couple Leisure'])
df.loc[conditions, 'Type_of_flight'] = df.loc[conditions, 'Traveller_type']
df['Traveller_type'] = df['Traveller_type'].where(~conditions, '')

df

# Checking the unique values in the Traveller_type column
unique_values_column = df['Type_of_flight'].unique()
print(unique_values_column)

# Create a new column 'class_copy2' and initialize it with NaN
df['Class_copy2'] = np.nan

# Move values from Type_of_flight to 'class_copy2' based on conditions
conditions = df['Type_of_flight'].isin(['Business', 'Business Class', 'First Class','Economy Class', 'Premium Economy', 'no'])
df.loc[conditions, 'Class_copy2'] = df.loc[conditions, 'Type_of_flight']
df.loc[conditions, 'Type_of_flight'] = np.nan

df

# Checking the unique values in the Traveller_type column
unique_values_column = df['Class_copy2'].unique()
print(unique_values_column)

# Count occurrences of a particular value in the 'Class_copy2' column
value_counts = df['Class_copy2'].value_counts()
print(value_counts)

# Print the count of a specific value, for example, 'Solo Leisure'
print("Count of 'no':", value_counts.get('no', 0))

df.to_csv(r'C:\Users\RRK\Desktop\Northumbria\ML Assesment\new1.csv')

"""# Merging the columns"""

# Replace missing values in 'Merged_route' with values from 'Route_copy' only if 'Merged_route' is NaN or None
df['Merged_route'] = df.apply(lambda row: row['Route_copy'] if pd.isnull(row['Route']) else row['Route'], axis=1)

df

df.to_csv(r'new9.csv')

# Copy 'yes' and 'no' values from 'Merged_route' to 'Yes_No_copy' column
mask_yes = df['Merged_route'] == 'yes'
mask_no = df['Merged_route'] == 'no'

# Copy 'yes' and 'no' values from 'Merged_route' column to 'Yes_No_copy' column
df.loc[mask_yes, 'Yes_No_copy'] = 'yes'
df.loc[mask_no, 'Yes_No_copy'] = 'no'

# Replace 'Merged_route' values that are 'no' with corresponding 'Route_copy2' values
mask = df['Merged_route'] == 'no'
df.loc[mask, 'Merged_route'] = df.loc[mask, 'Route_copy2']

df.to_csv(r'new9.csv')

# Copy values from 'Flying_month_copy' to 'Flying_month' where 'Flying_month' is missing
mask_missing = df['Flying_month'].isnull()  # Filter for missing values in 'Flying_month'
df.loc[mask_missing, 'Flying_month'] = df['Flying_month_copy']

# Check for missing values in 'Traveller_type' and copy values from 'traveller_type_copy' column
mask_missing_traveller = df['Traveller_type'].isnull()  # Filter for missing values in 'Traveller_type'
df.loc[mask_missing_traveller, 'Traveller_type'] = df.loc[mask_missing_traveller, 'traveller_type_copy']

# Check the data types of the columns
print(df[['Traveller_type', 'traveller_type_copy']].dtypes)

df.to_csv(r'new12.csv')

# Create a new column 'Merge_class' with values from 'Class' and 'Class_copy'
df['Merge_class'] = df['Class']
df.loc[df['Class'].isnull(), 'Merge_class'] = df['Class_copy']

df.to_csv(r'new13.csv')

# Create a new column and copy values from 'Traveller_type' and 'traveller_type_copy'
# Copy values from 'traveller_type_copy' to 'Merge_traveller_type' where 'Merge_traveller_type' is missing
df['Merge_traveller_type'] = df['Traveller_type'].fillna(df['traveller_type_copy'])
df['Merge_traveller_type'] = df['Traveller_type']
mask = df['Merge_traveller_type'].isnull() | df['Merge_traveller_type'].eq('')

df.loc[mask, 'Merge_traveller_type'] = df.loc[mask, 'traveller_type_copy']

df

df.to_csv(r'new14.csv')

df

# Create a new column 'Merge_class' and copy values from 'Class' and 'Class_copy'
# Copy values from 'Class_copy' to 'Merge_class' where 'Merge_class' value is missing
df['Merge_class'] = df['Class']
mask_class = df['Merge_class'].isnull() | df['Merge_class'].eq('') | (df['Merge_class'] == 'None')

df.loc[mask_class, 'Merge_class'] = df.loc[mask_class, 'Class_copy']

df

mask_class = df['Merge_class'].isnull() | df['Merge_class'].eq('') | (df['Merge_class'] == 'None') | (df['Merge_class'] == 'NaN')

df.loc[mask_class, 'Merge_class'] = df.loc[mask_class, 'Class_copy2']

df

df.to_csv(r'new15.csv')

# List of columns to drop
columns_to_drop = ['Route', 'Traveller_type',	'Class',	'Route_copy',	'Flying_month_copy',	'Class_copy',	'traveller_type_copy',	'Route_copy2',	'Class_copy2']

# Dropping multiple columns
df.drop(columns=columns_to_drop, inplace=True)

df

# Dictionary to map old column names to new column names
new_column_names = {
    'Merged_route': 'Route',
    'Merge_traveller_type': 'Traveller_type',
    'Merge_class': 'Class',
    'Passanger_Name': 'Passenger_name'
}

# Renaming multiple columns
df.rename(columns=new_column_names, inplace=True)

df

df.to_csv(r'new16.csv')

df.isna().sum()

# Regular expression pattern to find mentions of various date formats
date_formats = [
    r'\b\d{1,2}\s(?:January|February|March|April|May|June|July|August|September|October|November|December)\s\d{2,4}\b',  # 14 September 2023
    r'\b\d{1,2}/\d{1,2}/\d{2,4}\b',  # 14/09/2023 or 14/9/23
    r'\b\d{1,2}\.\d{1,2}\.\d{2,4}\b',  # 14.9.23
    r'\b\d{4}-\d{1,2}-\d{1,2}\b',  # 2023-09-14
    r'\b\d{1,2}-\w{3}-\d{2,4}\b',  # 14-Sep-2023 or 14-Sep-23
    r'\b\d{1,2}-\w{3}\b'  # 14-Sep
]

# Combine all date format patterns
combined_pattern = '|'.join(date_formats)

# Extracting mentions of dates from the 'Review' column
df['Date_Mentioned'] = df['Review_content'].apply(lambda x: re.findall(combined_pattern, x))

print(df[['Review_content', 'Date_Mentioned']])

df.to_csv(r'new17.csv')

# Count the occurrences of each date mention in the 'Date_Mentioned' column
date_counts = df['Date_Mentioned'].explode().value_counts()

print(date_counts)

# Count the total number of non-null values in the 'Date_Mentioned' column
total_values = df['Date_Mentioned'].count()

print("Total number of values in 'Date_Mentioned' column:", total_values)

# Count the number of non-empty list values in the 'Date_Mentioned' column
Date_mentioned_count = df[df['Date_Mentioned'].apply(lambda x: isinstance(x, list) and len(x) > 0)].shape[0]

print("Number of non-empty list values in 'Date_mentioned' column:", Date_mentioned_count)

df

print (type(df.loc[df.index[10], 'Date_Mentioned']))

print (type(df.loc[10, 'Date_Mentioned']))

df['Date_Mentioned'] = df['Date_Mentioned'].str.get(0)

df

df['Date_Mentioned'].unique()

df.dtypes

df.info()

# Convert to datetime
df['Date_Mentioned'] = pd.to_datetime(df['Date_Mentioned'], errors='coerce')

# Convert to the desired format "Jun-23"
df['Date_Mentioned'] = df['Date_Mentioned'].dt.strftime('%b-%y')

df['Date_Mentioned'].unique()

df.info()

na_values = df['Flying_month'].isnull().sum()
print("Number of NaT values in 'Flying_month' column:", na_values)

df.info()

df.to_csv(r'new18.csv')

# Copy values from 'Date_Mentioned' to 'Flying_month' where 'Flying_month' value is missing
mask_class = df['Flying_month'].isnull() | df['Flying_month'].eq('') | (df['Flying_month'] == 'None')

df.loc[mask_class, 'Flying_month'] = df.loc[mask_class, 'Date_Mentioned']

df

df.to_csv(r'new19.csv')

df.info()

df.isnull().sum()

from datetime import datetime

df['Flying_month'].fillna('', inplace=True)

# Converting date strings to datetime objects and storing them in a new column
df['Formatted_date'] = df['Flying_month'].apply(lambda x: datetime.strptime(x, '%b-%y') if x != '' else pd.NaT)

df.info()

df.to_csv(r'new20.csv')

columns_to_drop = ['Date_Mentioned','Flying_month']
df.drop(columns=columns_to_drop, inplace=True)

df.rename(columns={'Formatted_date': 'Flying_month'}, inplace=True)

df.info()

# Change the Position of a Columns
new_cols = ['Passenger_name','Flying_month','Route','Class','Traveller_type','Type_of_flight','Rating','Verified','Review_title','Review_content','Yes_No_copy']
df = df[new_cols]

df.info()

df.rename(columns={'Yes_No_copy': 'Yes_&_No'}, inplace=True)

df.info()

df.to_csv(r'new21.csv')

# Change the Position of a Columns
new_cols = ['Passenger_name','Flying_month','Route','Class','Traveller_type','Type_of_flight','Rating','Verified','Yes_&_No','Review_title','Review_content']
df = df[new_cols]

df.info()

df.to_csv(r'new22.csv')

# Copy 'No' values from 'Class' column to 'Yes_&_No' column where 'Yes_&_No' column is empty
mask = df['Yes_&_No'].isnull()  # Identify rows with NaN in 'Yes_&_No' column
no_values = df['Class'] == 'no'  # Identify rows in 'Class' column with 'No' values

# Replace NaN in 'Yes_&_No' column with 'No' values from 'Class' column
df.loc[mask & no_values, 'Yes_&_No'] = 'no'

# Replace values in 'Class' column
df['Class'].replace({'Business': 'Business Class', 'Premium Economy': 'Premium Economy Class'}, inplace=True)

# Replace values in 'Yes_&_No' column
df['Yes_&_No'].replace({'no': 'No', 'yes': 'Yes'}, inplace=True)

df.to_csv(r'new22.csv')

# Replace specific values (like None, empty strings, etc.) with np.nan in multiple columns
columns_to_replace = ['Flying_month', 'Route', 'Class','Traveller_type','Type_of_flight','Rating','Verified','Yes_&_No']
values_to_replace = ['None', '', 'no', 'Various']  # Include values to be replaced

df[columns_to_replace] = df[columns_to_replace].replace(values_to_replace, np.nan)

df

df.info()

df.to_csv(r'new22.csv')

# Convert 'Flying_month' column to datetime format
df['Flying_month'] = pd.to_datetime(df['Flying_month'])

# Create a new column 'Formatted_month' with 'Jan 2023' format
df['Formatted_month'] = df['Flying_month'].dt.strftime('%b %Y')

# Drop the 'Flying_month' column
df.drop(columns=['Flying_month'], inplace=True)

# Rename 'Formatted_month' column to 'Flying_month'
df.rename(columns={'Formatted_month': 'Flying_month'}, inplace=True)

# Change the Position of a Columns
new_cols = ['Passenger_name','Flying_month','Route','Class','Traveller_type','Type_of_flight','Rating','Verified','Yes_&_No','Review_title','Review_content']
df = df[new_cols]

df

df.info()

df.to_csv(r'new30.csv')

"""# **EDA**

# **1. Checking Missing Data**
"""

df.isnull().sum()

df.isnull().sum().plot(kind= 'bar')
plt.xlabel('Features')
plt.ylabel("Total Number of NaN values")

df.describe(include="all")

#Categorical Dataset Describe
df.describe(exclude=float)

df.rename(columns={'Yes_&_No': 'Recommended'}, inplace=True)
df['Recommended'].value_counts()

df.dtypes

"""# **2. Data Distribution**"""

# Plot histograms for each numerical column
for column in df.select_dtypes(include=['int64', 'float64']):
    plt.figure(figsize=(8, 4))
    plt.hist(df[column], bins=10, alpha=0.7)
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.title(f'Distribution of {column}')
    plt.show()

"""# **3. Outliers**"""

from scipy import stats

def find_outliers_zscore(data):
    z_scores = stats.zscore(data)
    threshold = 3
    return (abs(z_scores) > threshold)

# Create a boxplot with outliers highlighted using Seaborn
plt.figure(figsize=(12, 8))

# Loop through each column in the DataFrame
for i, column in enumerate(df.columns):
    if df[column].dtype in ['int64', 'float64']:  # Consider numerical columns only
        plt.subplot(len(df.columns)//2, 2, i+1)

# Find outliers using z-score method
        outliers = find_outliers_zscore(df[column])

# Create boxplot with outliers highlighted
        sns.boxplot(x=df[column])
        sns.scatterplot(x=df[column][outliers], y=df[column][outliers], color='red', label='Outliers')

        plt.title(f'Boxplot of {column}')
        plt.legend()

plt.tight_layout()
plt.show()

"""# **4. Correlation & Pattern**"""

# Calculate correlation matrix
correlation_matrix = df.corr()

# Display correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Create a heatmap of the correlation matrix
plt.figure(figsize=(4, 2))
sns.heatmap(correlation_matrix, annot=True, cmap='Spectral', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""# **5. Data Visualization & Data Quality**"""

df.info()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

class_counts = df['Class'].value_counts()


colors = {'Economy Class': 'green', 'Premium Economy Class': 'yellow', 'Business Class': 'red', 'First Class': 'blue'}

# Pie chart
plt.figure(figsize=(12, 6))
df['Class'].value_counts().plot.pie(autopct='%1.0f%%', colors=[colors[type] for type in class_counts.index])
plt.title('Class Distribution')

# Bar chart
plt.figure(figsize=(10, 6))
class_counts.plot(kind='bar', color=[colors[type] for type in class_counts.index])

# Add count annotations above each bar
for i, count in enumerate(class_counts):
    plt.text(i, count + 0.05, str(count), ha='center', va='bottom')  # Adjust the va parameter to set the vertical alignment

# Set labels and title for bar chart
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Count of Each Class')

# Rotate x-axis labels to 0 degrees
plt.xticks(rotation=0)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is your DataFrame

# Calculate the total count for each category
traveller_type_counts = df['Traveller_type'].value_counts()

# Define colors for each traveler type
colors = {'Solo Leisure': 'green', 'Couple Leisure': 'red', 'Family Leisure': 'blue'}

# Pie chart
plt.figure(figsize=(12, 6))
df['Traveller_type'].value_counts().plot.pie(autopct='%1.0f%%', colors=[colors[type] for type in traveller_type_counts.index])
plt.title('Traveller Type Distribution')

# Bar chart
plt.figure(figsize=(10, 6))
traveller_type_counts.plot(kind='bar', color=[colors[type] for type in traveller_type_counts.index])

# Add count annotations above each bar
for i, count in enumerate(traveller_type_counts):
    plt.text(i, count + 0.05, str(count), ha='center', va='bottom')  # Adjust the va parameter to set the vertical alignment

# Set labels and title for bar chart
plt.xlabel('Traveller Type')
plt.ylabel('Count')
plt.title('Count of Each Traveller Type')
plt.xticks(rotation=0)

plt.show()

#plotting a correlation heatmap for all variables
plt.figure(figsize=(10,5))
dataplot = sns.heatmap(df.select_dtypes(include = np.number).corr(),annot=True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is your DataFrame

# Calculate the total count for each category
rating_counts = df['Rating'].value_counts().sort_index()

# Define a color palette for ratings (adjust as needed)
colors = sns.color_palette('viridis', n_colors=len(rating_counts))

# Pie chart
plt.figure(figsize=(12, 6))
df['Rating'].value_counts().plot.pie(autopct='%1.0f%%', colors=colors)
plt.title('Rating Distribution')

# Bar chart
plt.figure(figsize=(10, 6))
rating_counts.plot(kind='bar', color=colors)

# Add count annotations above each bar
for i, count in enumerate(rating_counts):
    plt.text(i, count + 0.05, str(count), ha='center', va='bottom')  # Adjust the va parameter to set the vertical alignment

# Set labels and title for bar chart
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Count of Each Rating')

# Rotate x-axis labels to 0 degrees
plt.xticks(rotation=0)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is your DataFrame

# Calculate the total count for each category
verified_counts = df['Verified'].value_counts()

# Define colors for each verification status
colors = {'Trip Verified': 'green', 'Not Verified': 'red'}

# Pie chart
plt.figure(figsize=(12, 6))
df['Verified'].value_counts().plot.pie(autopct='%1.0f%%', colors=[colors[type] for type in verified_counts.index])
plt.title('Verification Status Distribution')

# Bar chart
plt.figure(figsize=(10, 6))
verified_counts.plot(kind='bar', color=[colors[type] for type in verified_counts.index])

# Add count annotations above each bar
for i, count in enumerate(verified_counts):
    plt.text(i, count + 0.05, str(count), ha='center', va='bottom')  # Adjust the va parameter to set the vertical alignment

# Set labels and title for bar chart
plt.xlabel('Verification Status')
plt.ylabel('Count')
plt.title('Count of Each Verification Status')

# Rotate x-axis labels to 0 degrees
plt.xticks(rotation=0)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is your DataFrame

# Calculate the total count for each category
recommended_counts = df['Recommended'].value_counts()

# Define colors for each recommendation status
colors = {'Yes': 'green', 'No': 'red'}

# Pie chart
plt.figure(figsize=(12, 6))
df['Recommended'].value_counts().plot.pie(autopct='%1.0f%%', colors=[colors[type] for type in recommended_counts.index])
plt.title('Recommendation Status Distribution')

# Bar chart
plt.figure(figsize=(10, 6))
recommended_counts.plot(kind='bar', color=[colors[type] for type in recommended_counts.index])

# Add count annotations above each bar
for i, count in enumerate(recommended_counts):
    plt.text(i, count + 0.05, str(count), ha='center', va='bottom')  # Adjust the va parameter to set the vertical alignment

# Set labels and title for bar chart
plt.xlabel('Recommendation Status')
plt.ylabel('Count')
plt.title('Count of Each Recommendation Status')
plt.xticks(rotation=0)

plt.show()

Review_content_length = df.Review_content.str.len()

type(Review_content_length)

# finding the reviews with max length
max(Review_content_length)

# finding the reviews with min length
min(Review_content_length)

"""# **Sentiment Analysis**"""

from wordcloud import WordCloud

#'df' is the DataFrame and 'Review_content' is the column containing text data
text_data = ' '.join(df['Review_content'].dropna())  # Concatenate text data into a single string

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

# Display the word cloud using Matplotlib
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Hide axis
plt.title('Word Cloud of Most Common Words')
plt.show()

# 'df' is the DataFrame and 'Review_content' is the column containing reviews
reviews = df['Review_content'].dropna()  # Retrieve the reviews column

# Calculate the length of each review
review_lengths = reviews.str.len()

# Plotting the distribution of character lengths with KDE plot and different colors
plt.figure(figsize=(18, 6))
sns.histplot(review_lengths, bins=100, kde=True, color='red', edgecolor='black')
sns.histplot(review_lengths, bins=100, kde=False, color='cyan')
plt.xlabel('Number of Characters')
plt.ylabel('Frequency')
plt.title('Distribution of Character Lengths in Reviews_content')
plt.grid(True)
plt.show()

pip install nltk

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from collections import Counter

# 'df' is the DataFrame and 'Review_content' is the column containing reviews
reviews = df['Review_content'].dropna()  # Retrieve the 'Review_content' column

# Tokenize the reviews into words
words = []
for review in reviews:
    words.extend(word_tokenize(review.lower()))  # Convert to lowercase

# Count the frequency of each word
word_freq = Counter(words)

# Get the top 50 most common words
top_50_words = dict(word_freq.most_common(50))

# Create a horizontal bar chart
plt.figure(figsize=(12, 10))
plt.barh(list(top_50_words.keys()), list(top_50_words.values()), color='skyblue')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.title('Top 50 Most Used Words in Review Content')
plt.gca().invert_yaxis()  # Invert y-axis to show most common words on top
plt.show()

nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('opinion_lexicon')

from nltk.corpus import opinion_lexicon
# Load positive and negative words from NLTK's opinion lexicon
positive_words = set(opinion_lexicon.positive())
negative_words = set(opinion_lexicon.negative())

# 'df' is the DataFrame and 'Review_content' is the column containing reviews
reviews = df['Review_content'].dropna()  # Retrieve the 'Review_content' column

# Tokenize the reviews into words
words = []
for review in reviews:
    words.extend(word_tokenize(review.lower()))  # Convert to lowercase

# Count the frequency of each word
word_freq = Counter(words)

# Extract counts of positive and negative words
positive_word_freq = {word: word_freq[word] for word in positive_words if word in word_freq}
negative_word_freq = {word: word_freq[word] for word in negative_words if word in word_freq}

# Get the top 20 most common positive and negative words
top_positive_words = dict(Counter(positive_word_freq).most_common(20))
top_negative_words = dict(Counter(negative_word_freq).most_common(20))

# Create separate graphs for top positive and negative words
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Positive words graph
axes[0].barh(list(top_positive_words.keys()), list(top_positive_words.values()), color='green', alpha=0.7)
axes[0].set_xlabel('Frequency')
axes[0].set_ylabel('Words')
axes[0].set_title('Top 20 Most Common Positive Words')

# Negative words graph
axes[1].barh(list(top_negative_words.keys()), list(top_negative_words.values()), color='red', alpha=0.7)
axes[1].set_xlabel('Frequency')
axes[1].set_ylabel('Words')
axes[1].set_title('Top 20 Most Common Negative Words')

plt.tight_layout()
plt.show()

# Load positive and negative words from NLTK's opinion lexicon
positive_words = set(opinion_lexicon.positive())
negative_words = set(opinion_lexicon.negative())

# 'df' is the DataFrame and 'Review_content' is the column containing reviews
reviews = df['Review_content'].dropna()  # Retrieve the 'Review_content' column

# Tokenize the reviews into words
words = []
for review in reviews:
    words.extend(word_tokenize(review.lower()))  # Convert to lowercase

# Count the frequency of each word
word_freq = Counter(words)

# Extract counts of positive and negative words
positive_word_freq = {word: word_freq[word] for word in positive_words if word in word_freq}
negative_word_freq = {word: word_freq[word] for word in negative_words if word in word_freq}

# Calculate total positive and negative words count
total_positive_words = sum(positive_word_freq.values())
total_negative_words = sum(negative_word_freq.values())

# Calculate percentage of top positive and negative words
top_positive_words = dict(Counter(positive_word_freq).most_common(20))
top_negative_words = dict(Counter(negative_word_freq).most_common(20))

top_positive_words_percentage = {word: (count / total_positive_words) * 100 for word, count in top_positive_words.items()}
top_negative_words_percentage = {word: (count / total_negative_words) * 100 for word, count in top_negative_words.items()}

# Create separate graphs for top positive and negative words percentages
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Positive words percentage graph
axes[0].barh(list(top_positive_words_percentage.keys()), list(top_positive_words_percentage.values()), color='green', alpha=0.7)
axes[0].set_xlabel('Word Prevalence (% of words in Review_content)')
axes[0].set_ylabel('Words')
axes[0].set_title('Top 20 Most Common Positive Words (%)')

# Negative words percentage graph
axes[1].barh(list(top_negative_words_percentage.keys()), list(top_negative_words_percentage.values()), color='red', alpha=0.7)
axes[1].set_xlabel('Word Prevalence (% of words in Review_content)')
axes[1].set_ylabel('Words')
axes[1].set_title('Top 20 Most Common Negative Words (%)')

plt.tight_layout()
plt.show()

from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initialize the sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to classify sentiment based on score
def classify_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Analyze sentiment for each review and classify
def get_sentiment_counts(df):
    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for index, row in df.iterrows():
        sentiment_score = sia.polarity_scores(row['Review_content'])['compound']
        sentiment = classify_sentiment(sentiment_score)

        if sentiment == 'Positive':
            positive_count += 1
        elif sentiment == 'Negative':
            negative_count += 1
        else:
            neutral_count += 1

    return positive_count, negative_count, neutral_count

# Get counts of positive, negative, and neutral reviews
positive_reviews, negative_reviews, neutral_reviews = get_sentiment_counts(df)

print(f"Positive reviews: {positive_reviews}")
print(f"Negative reviews: {negative_reviews}")
print(f"Neutral reviews: {neutral_reviews}")

get_sentiment_counts(df)

# Visualization - Bar plot

# Calculate total number of reviews
total_reviews = len(df)

# Calculate percentages
positive_percentage = (positive_reviews / total_reviews) * 100
negative_percentage = (negative_reviews / total_reviews) * 100
neutral_percentage = (neutral_reviews / total_reviews) * 100

# Visualization - Two subplots (counts and percentages)
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Plot 1: Bar plot of counts with labels
sentiments = ['Positive', 'Negative', 'Neutral']
counts = [positive_reviews, negative_reviews, neutral_reviews]

bars = axes[0].bar(sentiments, counts, color=['green', 'red', 'yellow'])
axes[0].set_xlabel('Sentiments')
axes[0].set_ylabel('Number of Reviews')
axes[0].set_title('Sentiment Distribution - Counts')

# Attach counts as text labels on each bar
for bar, count in zip(bars, counts):
    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),
                 ha='center', va='bottom', fontsize=10)

# Plot 2: Bar plot of percentages with labels
percentages = [positive_percentage, negative_percentage, neutral_percentage]

bars = axes[1].bar(sentiments, percentages, color=['green', 'red', 'yellow'])
axes[1].set_xlabel('Sentiments')
axes[1].set_ylabel('Percentage of Reviews')
axes[1].set_title('Sentiment Distribution - Percentages')

# Attach percentages as text labels on each bar
for bar, percent in zip(bars, percentages):
    axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{percent:.2f}%',
                 ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

df.info()

df

"""# **Creating new dataframe name df_Class for doing analysis based on Class column values those are available**

# **•	Research Question 1: Is there any impact of the class of travel on customer experience?**


1.   Let us first compare the class with ratings
"""

df_Class=  df.dropna(subset=['Class', 'Rating'])

df_Class.info()

pip install scipy

# Box plot to visualize the impact of class on ratings
plt.figure(figsize=(8, 6))
df_Class.boxplot(column='Rating', by='Class', grid=False)
plt.title('Impact of Class on Customer Ratings')
plt.xlabel('Class')
plt.ylabel('Ratings')
plt.show()

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame
df['Rating'] = df['Rating'].astype(float)  # Convert Rating column to float if not already

# Plotting the histogram
plt.hist(df['Rating'], bins=5, edgecolor='black')  # You can adjust the number of bins as needed
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

import scipy
from scipy.stats import f_oneway
# Perform ANOVA test to compare ratings among different travel classes
grouped_data = [df_Class[df_Class['Class'] == cls]['Rating'] for cls in df_Class['Class'].unique()]
f_statistic, p_value = f_oneway(*grouped_data)
print(f"ANOVA Test - F-statistic: {f_statistic}, p-value: {p_value}")

import pandas as pd
from scipy.stats import kruskal

# Perform Kruskal-Wallis test
result = kruskal(*[group["Rating"].values for name, group in df_Class.groupby("Class")])

print("\nKruskal-Wallis Test Result:")
print("H-statistic:", result.statistic)
print("P-value:", result.pvalue)
# Set the significance level
alpha = 0.05

if result.pvalue < alpha:
    print("\nReject the null hypothesis. There is a significant difference in medians.")
else:
    print("\nFail to reject the null hypothesis. No significant difference in medians.")
medians_by_group = df_Class.groupby("Class")["Rating"].median()
print("\nMedians by Group:")
print(medians_by_group)



print(df_Class.isnull().sum())

from scipy.stats import chi2_contingency


# Create a contingency table between 'Travel_Class' and 'Rating'
contingency_table = pd.crosstab(df_Class['Class'], df_Class['Rating'])

# Perform chi-square test of independence
chi2, p_value, _, _ = chi2_contingency(contingency_table)
print(f"Chi-square test statistic: {chi2}, p-value: {p_value}")

# Extract ratings for each class
ratings_economy = df_Class[df_Class['Class'] == 'Economy Class']['Rating']
ratings_premium_economy = df_Class[df_Class['Class'] == 'Premium Economy Class']['Rating']
ratings_business = df_Class[df_Class['Class'] == 'Business Class']['Rating']
ratings_first_class = df_Class[df_Class['Class'] == 'First Class']['Rating']

# Perform ANOVA test
f_statistic, p_value = f_oneway(ratings_economy, ratings_premium_economy, ratings_business, ratings_first_class)

# Interpret the results
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis. There is a significant difference in average ratings among classes.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in average ratings among classes.")

# Calculate average ratings for each class
avg_ratings = df_Class.groupby('Class')['Rating'].mean().sort_values()

# Bar graph to visualize average ratings per class
plt.figure(figsize=(8, 6))
avg_ratings.plot(kind='bar', color='skyblue')
plt.title('Average Ratings per Class')
plt.xlabel('Class')
plt.ylabel('Average Rating')
plt.xticks(rotation=0)
plt.show()

# Calculate average ratings for each class
avg_ratings = df_Class.groupby('Class')['Rating'].mean().sort_values()

# Define colors for each bar
colors = ['skyblue', 'orange', 'silver', 'gold']  # Add more colors as needed for each class

# Bar graph to visualize average ratings per class with different colors
plt.figure(figsize=(8, 6))
avg_ratings.plot(kind='bar', color=colors)
plt.title('Average Ratings per Class')
plt.xlabel('Class')
plt.ylabel('Average Rating')
plt.xticks(rotation=0)

# Add text labels for each bar
for index, value in enumerate(avg_ratings):
    plt.text(index, value, f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Calculate average ratings for each class
avg_ratings = df_Class.groupby('Class')['Rating'].mean().sort_values()

# Bar graph to visualize average ratings per class
plt.figure(figsize=(8, 6))
avg_ratings.plot(kind='bar', color='turquoise')
plt.title('Average Ratings per Class')
plt.xlabel('Class')
plt.ylabel('Average Rating')
plt.xticks(rotation=0)

# Add text labels for each bar
for index, value in enumerate(avg_ratings):
    plt.text(index, value, f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Pivot the DataFrame to create a heatmap
heatmap_data = df_Class.pivot_table(index='Class', columns='Rating', aggfunc='size', fill_value=0)

# Heatmap to visualize the distribution of ratings across classes
plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='d')
plt.title('Rating Distribution across Classes')
plt.xlabel('Rating')
plt.ylabel('Class')
plt.show()

# Calculate percentage of total ratings for each class
rating_counts = df_Class['Class'].value_counts(normalize=True) * 100

# Pie chart to visualize percentage of total ratings contributed by each class
plt.figure(figsize=(8, 6))
plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
plt.axis('equal')
plt.title('Percentage of Total Ratings by Class')
plt.show()

# Calculate percentage of total ratings for each class
rating_counts = df_Class['Class'].value_counts(normalize=True) * 100

# Define the order of classes
class_order = ['Economy Class', 'Premium Economy Class', 'Business Class', 'First Class']

# Reorder the rating_counts Series based on the defined order
rating_counts = rating_counts[class_order]

# Create a pie chart with the ordered data
plt.figure(figsize=(8, 6))
plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
plt.axis('equal')
plt.title('Percentage of Total Ratings by Class')
plt.show()

"""2.   Second compare class with reviews with respect to sentimental analysis for which class has more positive, negative and neutral travel experience

"""

# Initialize the sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to classify sentiment based on score
def classify_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Perform sentiment analysis for each class separately
sentiment_results = {}
for class_name in df_Class['Class'].unique():
    class_reviews = df_Class[df_Class['Class'] == class_name]['Review_content']

    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for review in class_reviews:
        sentiment_score = sia.polarity_scores(review)['compound']
        sentiment = classify_sentiment(sentiment_score)

        if sentiment == 'Positive':
            positive_count += 1
        elif sentiment == 'Negative':
            negative_count += 1
        else:
            neutral_count += 1

    sentiment_results[class_name] = {'Positive': positive_count, 'Negative': negative_count, 'Neutral': neutral_count}

# Display sentiment analysis results for each class
for class_name, sentiment_counts in sentiment_results.items():
    print(f"Sentiment Analysis for {class_name}:")
    for sentiment, count in sentiment_counts.items():
        print(f"{sentiment}: {count}")
    print()

# Drop rows with NaN values in 'Class' column
df_Class.dropna(subset=['Class'], inplace=True)

# Initialize the sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to classify sentiment based on score
def classify_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Perform sentiment analysis for each class separately
sentiment_results = {}
for class_name in df_Class['Class'].unique():
    class_reviews = df_Class[df_Class['Class'] == class_name]['Review_content']

    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for review in class_reviews:
        sentiment_score = sia.polarity_scores(review)['compound']
        sentiment = classify_sentiment(sentiment_score)

        if sentiment == 'Positive':
            positive_count += 1
        elif sentiment == 'Negative':
            negative_count += 1
        else:
            neutral_count += 1

    sentiment_results[class_name] = {'Positive': positive_count, 'Negative': negative_count, 'Neutral': neutral_count}

# Display sentiment analysis results for each class
for class_name, sentiment_counts in sentiment_results.items():
    print(f"Sentiment Analysis for {class_name}:")
    for sentiment, count in sentiment_counts.items():
        print(f"{sentiment}: {count}")
    print()

# Convert sentiment analysis results to DataFrame
sentiment_df = pd.DataFrame(sentiment_results).T

# Plotting bar graphs for sentiment analysis results
sentiment_df.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Sentiment Analysis for Each Class')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Sentiment')
plt.show()

# Plotting vertical bar graphs for sentiment analysis results
sentiment_df.plot(kind='bar', figsize=(10, 6), width=0.8)
plt.title('Sentiment Analysis for Each Class')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Class')
plt.show()

# Plotting vertical bar graphs for sentiment analysis results with labels
ax = sentiment_df.plot(kind='bar', figsize=(10, 6), width=0.8)
plt.title('Sentiment Analysis for Each Class')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Class')

# Adding labels to the bars
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')

plt.show()

# Calculate correlation matrix
correlation_matrix = sentiment_df.corr()

# Plotting the correlation matrix as a heatmap
plt.figure(figsize=(8, 6))
plt.title('Correlation Matrix for Sentiment Analysis')
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

# Transpose the sentiment analysis DataFrame
sentiment_df_transposed = sentiment_df.T

# Create a correlation matrix between sentiment analysis and classes
correlation_matrix = sentiment_df_transposed.corr()

print("Correlation Matrix between Sentiment Analysis and Classes:")
print(correlation_matrix)

correlation_matrix

# Plotting the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='Spectral', fmt=".3f")
plt.title('Correlation Matrix between Sentiment Analysis and Classes')
plt.xlabel('Class')
plt.ylabel('Class')
plt.show()

# Initialize the sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to classify sentiment based on score
def classify_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Perform sentiment analysis for each class separately
sentiment_results = {}
for class_name in df_Class['Class'].unique():
    class_reviews = df_Class[df_Class['Class'] == class_name]['Review_content']

    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for review in class_reviews:
        sentiment_score = sia.polarity_scores(review)['compound']
        sentiment = classify_sentiment(sentiment_score)

        if sentiment == 'Positive':
            positive_count += 1
        elif sentiment == 'Negative':
            negative_count += 1
        else:
            neutral_count += 1

    total_reviews = positive_count + negative_count + neutral_count
    sentiment_results[class_name] = {
        'Positive': (positive_count / total_reviews) * 100,
        'Negative': (negative_count / total_reviews) * 100,
        'Neutral': (neutral_count / total_reviews) * 100
    }

# Convert sentiment analysis results to DataFrame
sentiment_df_Class = pd.DataFrame(sentiment_results).T.reset_index().rename(columns={'index': 'Class'})

# Melt the DataFrame for visualization
melted_sentiment_df_Class = sentiment_df_Class.melt(id_vars='Class', var_name='Sentiment', value_name='Percentage')

# Plotting the grouped bar plot for percentage distribution
plt.figure(figsize=(10, 6))
sns.barplot(x='Class', y='Percentage', hue='Sentiment', data=melted_sentiment_df_Class, palette='coolwarm')
plt.title('Percentage Distribution of Sentiment Analysis for Each Class')
plt.xlabel('Class')
plt.ylabel('Percentage')
plt.legend(title='Sentiment')
plt.show()

sentiment_df_Class

# Plotting the heatmap of percentage distribution
plt.figure(figsize=(8, 6))
sns.heatmap(sentiment_df_Class.set_index('Class'), annot=True, cmap='YlGnBu', fmt=".2f")
plt.title('Percentage Distribution of Sentiment Analysis for Each Class')
plt.xlabel('Sentiment')
plt.ylabel('Class')
plt.show()

# Plotting pie charts for each class
fig, axs = plt.subplots(2, 2, figsize=(12, 8))
axs = axs.flatten()

for i, class_name in enumerate(sentiment_df_Class['Class']):
    data = sentiment_df_Class.iloc[i, 1:].values
    labels = sentiment_df_Class.columns[1:]

    axs[i].pie(data, labels=labels, autopct='%1.1f%%', startangle=90, colors=['seagreen','red','turquoise'])
    axs[i].set_title(f'Sentiment Distribution for {class_name}')

plt.tight_layout()
plt.show()

df_Class.info()

"""# **Creating new dataframe name df_Flying_month for doing analysis based on flying month column values are available**"""

df_Flying_month= df[df['Flying_month'].notnull()].copy()

df_Flying_month.info()

"""# **•	Research Question 2: Does month of travel affect customer’s ratings?**"""

from scipy.stats import f_oneway

# Box plot of ratings by month
plt.figure(figsize=(10, 6))
sns.boxplot(x='Flying_month', y='Rating', data=df_Flying_month)
plt.title('Distribution of Ratings by Month of Travel')
plt.xlabel('Month of Travel')
plt.ylabel('Customer Rating')
plt.show()

# ANOVA test
grouped_data = [df_Flying_month[df_Flying_month['Flying_month'] == month]['Rating'] for month in df_Flying_month['Flying_month'].unique()]
f_stat, p_value = f_oneway(*grouped_data)
print(f'ANOVA Test - F-statistic: {f_stat}, p-value: {p_value}')

# Plot average Ratings per Flying_month
Flying_monthly_avg_Ratings = df_Flying_month.groupby('Flying_month')['Rating'].mean()

plt.figure(figsize=(18, 6))
Flying_monthly_avg_Ratings.plot(kind='line', marker='o')
plt.title('Average Ratings Over Time')
plt.xlabel('Flying_month')
plt.ylabel('Average Rating')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Convert 'Flying_month' to datetime format
df_Flying_month['Flying_month'] = pd.to_datetime(df_Flying_month['Flying_month'], format='%b %Y')

# Extract month as numerical value
df_Flying_month['Numeric_month'] = df_Flying_month['Flying_month'].dt.month

# Calculate correlation between 'Numeric_month' and 'Rating'
correlation_coefficient = df_Flying_month['Numeric_month'].corr(df_Flying_month['Rating'])
print(f"Correlation coefficient between 'Flying_month' and 'Rating': {correlation_coefficient}")

# Perform ANOVA to test if there are significant Rating differences among months
grouped_data = [df_Flying_month[df_Flying_month['Flying_month'].dt.month == m]['Rating'] for m in range(1, 13)]
f_statistic, p_value = f_oneway(*grouped_data)
print(f"ANOVA F-statistic: {f_statistic}, p-value: {p_value}")

# Perform ANOVA test for comparing 'Rating' among different months
# Example: Categorizing 'Flying_month' into months (1-12)
month_categories = [df_Flying_month[df_Flying_month['Flying_month'].dt.month == i]['Rating'] for i in range(1, 13)]

# Perform ANOVA test
f_statistic, p_value = f_oneway(*month_categories)

# Interpret the results
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis. There is a significant difference in average Rating among different months.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in average Rating among different months.")

# Drop rows with missing values in 'Flying_month' and 'Rating' columns
df_Flying_month.dropna(subset=['Flying_month', 'Rating'], inplace=True)

# Convert 'Flying_month' to month (numeric representation)
df_Flying_month['month_numeric'] = pd.to_datetime(df_Flying_month['Flying_month']).dt.month

# Calculate correlation between 'month_numeric' and 'Rating'
correlation = df_Flying_month['month_numeric'].corr(df_Flying_month['Rating'])
print(f"Correlation between 'Flying_month' and 'Rating': {correlation}")

# Create a heatmap
data = df_Flying_month[['month_numeric', 'Rating']]
heatmap_data = data.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 12})
plt.title('Correlation Heatmap: Flying_month vs Rating')
plt.xlabel('Features')
plt.ylabel('Features')
plt.show()

# Drop rows with missing values in 'Flying_month' and 'Rating' columns
df_Flying_month.dropna(subset=['Flying_month', 'Rating'], inplace=True)

# Convert 'Flying_month' to month (numeric representation)
df_Flying_month['month_numeric'] = pd.to_datetime(df_Flying_month['Flying_month']).dt.month

# Create a scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(df_Flying_month['month_numeric'], df_Flying_month['Rating'], alpha=0.5)
plt.title('Scatter Plot: Flying_month vs Rating')
plt.xlabel('Month (numeric)')
plt.ylabel('Rating')
plt.show()

# Drop rows with missing values in 'Rating' columns
df_Flying_month.dropna(subset=['Rating'], inplace=True)

# Convert 'Flying_month' to month (numeric representation)
df_Flying_month['month_numeric'] = pd.to_datetime(df_Flying_month['Flying_month']).dt.month

# Create a box plot
plt.figure(figsize=(8, 6))
sns.boxplot(x='month_numeric', y='Rating', data=df_Flying_month)
plt.title('Box Plot: Flying_month vs Rating')
plt.xlabel('Month (numeric)')
plt.ylabel('Rating')
plt.show()

# Drop rows with missing values in 'Flying_month' and 'Rating' columns
df_Flying_month.dropna(subset=['Flying_month', 'Rating'], inplace=True)

# Convert 'Flying_month' to month (numeric representation)
df_Flying_month['month_numeric'] = pd.to_datetime(df_Flying_month['Flying_month']).dt.month

# Calculate correlation between 'month_numeric' and 'Rating'
correlation = df_Flying_month['month_numeric'].corr(df_Flying_month['Rating'])
print(f"Correlation between 'Flying_month' and 'Rating': {correlation}")

# Create a heatmap
data = df_Flying_month[['month_numeric', 'Rating']]
heatmap_data = data.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 12})
plt.title('Correlation Heatmap: Flying_month vs Rating')
plt.xlabel('Features')
plt.ylabel('Features')
plt.show()

# Drop rows with missing values in 'Flying_month' and 'Rating' columns
df_Flying_month.dropna(subset=['Flying_month', 'Rating'], inplace=True)

# Convert 'Flying_month' to month (numeric representation)
df_Flying_month['month_numeric'] = pd.to_datetime(df_Flying_month['Flying_month']).dt.month

# Create a pivot table to show the relationship between 'Rating' and 'month_numeric'
pivot_table = df_Flying_month.pivot_table(index='Rating', columns='month_numeric', aggfunc='size', fill_value=0)

plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, annot=True, fmt='d', cmap='Paired')
plt.title('Relationship between Rating and Flying Month')
plt.xlabel('Month (Numeric)')
plt.ylabel('Rating')
plt.show()

# Drop rows with missing values in 'Flying_month' and 'Rating' columns
df_Flying_month.dropna(subset=['Flying_month', 'Rating'], inplace=True)

import pandas as pd
from scipy.stats import kruskal

# Extract Month from 'Flying_month'
df_Flying_month['Month'] = pd.to_datetime(df_Flying_month['Flying_month']).dt.month

# Perform Kruskal-Wallis test
result = kruskal(*[group['Rating'].values for name, group in df_Flying_month.groupby('Month')])

# Display the result
print("Kruskal-Wallis Test Result:")
print("H-statistic:", result.statistic)
print("P-value:", result.pvalue)

# Set the significance level
alpha = 0.05

# Interpret the results
if result.pvalue < alpha:
    print("\nReject the null hypothesis. There is a significant difference in ratings across months.")
else:
    print("\nFail to reject the null hypothesis. No significant difference in ratings across months.")

df_Flying_month.info()

"""# **Creating new dataframe name df_Traveller_type for doing analysis based on Traveller_type column values those are available**"""

df_Traveller_type= df[df['Traveller_type'].notnull()].copy()

df_Traveller_type.info()

"""# **•	Research Question 3: Has traveller type influence on customer’s review?**"""

# Bar plot to visualize reviews by traveller type
plt.figure(figsize=(8, 6))
sns.countplot(x='Traveller_type', data=df_Traveller_type)
plt.title('Reviews by Traveller Type')
plt.xlabel('Traveller Type')
plt.ylabel('Count of Reviews')
plt.show()

# Define the desired order for traveller types
desired_order = ['Solo Leisure', 'Couple Leisure', 'Family Leisure']

# Calculate count of reviews for each traveller type
count_reviews = df_Traveller_type['Traveller_type'].value_counts()

# Calculate percentage of reviews for each traveller type
percentage_reviews = df_Traveller_type['Traveller_type'].value_counts(normalize=True) * 100

# Create a figure with two subplots (side-by-side)
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))

# Plot 1: Count of reviews by traveller type in the desired order
sns.countplot(x='Traveller_type', data=df_Traveller_type, order=desired_order, ax=axes[0])
axes[0].set_title('Reviews by Traveller Type (Count)')
axes[0].set_xlabel('Traveller Type')
axes[0].set_ylabel('Count of Reviews')

# Display count values on top of bars
for p in axes[0].patches:
    axes[0].annotate(f'{p.get_height()}', (p.get_x()+p.get_width()/2, p.get_height()), ha='center', va='bottom')

# Plot 2: Percentage of reviews by traveller type in the desired order
percentage_reviews[desired_order].plot(kind='bar', ax=axes[1])
axes[1].set_title('Reviews by Traveller Type (%)')
axes[1].set_xlabel('Traveller Type')
axes[1].set_ylabel('% of Reviews')

# Display percentage values on top of bars
for i, v in enumerate(percentage_reviews[desired_order]):
    axes[1].text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom')

# Set x-axis tick labels rotation to horizontal
plt.xticks(range(len(desired_order)), desired_order, rotation=0)

plt.tight_layout()
plt.show()

# Define the desired order for traveller types and corresponding colors
desired_order = ['Solo Leisure', 'Couple Leisure', 'Family Leisure']
colors = ['skyblue', 'salmon', 'lightgreen']

# Calculate count of reviews for each traveller type
count_reviews = df_Traveller_type['Traveller_type'].value_counts()

# Calculate percentage of reviews for each traveller type
percentage_reviews = df_Traveller_type['Traveller_type'].value_counts(normalize=True) * 100

# Create a figure with two subplots (side-by-side)
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))

# Plot 1: Count of reviews by traveller type in the desired order with custom colors
sns.countplot(x='Traveller_type', data=df_Traveller_type, order=desired_order, ax=axes[0], palette=colors)
axes[0].set_title('Reviews by Traveller Type (Count)')
axes[0].set_xlabel('Traveller Type')
axes[0].set_ylabel('Count of Reviews')

# Display count values on top of bars
for i, p in enumerate(axes[0].patches):
    axes[0].annotate(f'{p.get_height()}', (p.get_x()+p.get_width()/2, p.get_height()), ha='center', va='bottom', color='black')

# Plot 2: Percentage of reviews by traveller type in the desired order with custom colors
percentage_reviews[desired_order].plot(kind='bar', ax=axes[1], color=colors)
axes[1].set_title('Reviews by Traveller Type (Percentage)')
axes[1].set_xlabel('Traveller Type')
axes[1].set_ylabel('Percentage of Reviews')

# Display percentage values on top of bars
for i, v in enumerate(percentage_reviews[desired_order]):
    axes[1].text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom')

# Set x-axis tick labels rotation to horizontal
plt.xticks(range(len(desired_order)), desired_order, rotation=0)

plt.tight_layout()
plt.show()

percentage_reviews

count_reviews

# Creating a new variable or column  named 'Sentiment' and using the NLTK package applying the sentiment analysis to observe each row of Review_content whether
# the given reviews is positive, negative, or neutral.
# Initialize SentimentIntensityAnalyzer from NLTK
sia = SentimentIntensityAnalyzer()

# Perform sentiment analysis for each row based on Traveller_type
def analyze_sentiment(row):
    sentiment_score = sia.polarity_scores(row['Review_content'])
    # Classify sentiment based on compound score
    if sentiment_score['compound'] >= 0.05:
        return 'Positive'
    elif sentiment_score['compound'] <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply sentiment analysis function to the dataframe
df_Traveller_type['Sentiment'] =df_Traveller_type.apply(analyze_sentiment, axis=1)

# Display a sample of the dataframe with the sentiment column
print(df_Traveller_type[['Traveller_type', 'Review_content', 'Sentiment']].head(10))

df_Traveller_type['Sentiment'].value_counts()

df_Traveller_type.info()

# Create a grouped bar plot for sentiment analysis by traveller type
plt.figure(figsize=(8, 6))
sns.countplot(x='Traveller_type', hue='Sentiment', data=df_Traveller_type, palette='viridis')
plt.title('Sentiment Analysis by Traveller Type')
plt.xlabel('Traveller Type')
plt.ylabel('Count of Reviews')
plt.legend(title='Sentiment')
plt.show()

# Countplot for sentiment analysis by traveller type
plt.figure(figsize=(12, 6))

# Countplot showing count values
plt.subplot(1, 2, 1)
sns.countplot(x='Traveller_type', hue='Sentiment', data=df_Traveller_type, palette='viridis')
plt.title('Sentiment Analysis by Traveller Type (Count)')
plt.xlabel('Traveller Type')
plt.ylabel('Count of Reviews')
plt.legend(title='Sentiment')

# Count values on top of bars
for p in plt.gca().patches:
    plt.gca().annotate(f'\n{int(p.get_height())}', (p.get_x()+p.get_width()/2, p.get_height()), ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Define the desired order for traveller types and corresponding colors
desired_order = ['Solo Leisure', 'Couple Leisure', 'Family Leisure']
colors = ['green','red', 'gray']

# Calculate percentage of sentiments for each traveller type
percentage_sentiments = df_Traveller_type.groupby('Traveller_type')['Sentiment'].value_counts(normalize=True).mul(100).rename('Percentage').reset_index()

# Create countplot displaying percentage values with specific order and colors
plt.figure(figsize=(12, 6))

sns.barplot(x='Traveller_type', y='Percentage', hue='Sentiment', data=percentage_sentiments, palette=colors, order=desired_order)
plt.title('Sentiment Analysis by Traveller Type (%)')
plt.xlabel('Traveller Type')
plt.ylabel('% of Reviews')
plt.legend(title='Sentiment', loc='upper right')

# Display percentage values on top of bars
for p in plt.gca().patches:
    plt.gca().annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2, p.get_height()),
                       ha='center', va='bottom', color='black', fontsize=8)

plt.tight_layout()
plt.show()

percentage_sentiments

# Create a contingency table between 'Traveller_type' and 'Sentiment'
contingency_table = pd.crosstab(df_Traveller_type['Traveller_type'], df_Traveller_type['Sentiment'])

# Perform Chi-square test of independence
chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2:.2f}")
print(f"P-value: {p:.4f}")
print(f"Degrees of freedom: {dof}")

# Create a contingency table between 'Traveller_type' and 'Sentiments'
contingency_table = pd.crosstab(df_Traveller_type['Traveller_type'], df_Traveller_type['Sentiment'])

# Perform the Chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# Interpret the results
alpha = 0.05
if p_val < alpha:
    print("Reject the null hypothesis. There is a significant association between Traveller_type and Sentiments.")
else:
    print("Fail to reject the null hypothesis. There is no significant association between Traveller_type and Sentiments.")

from matplotlib.colors import LinearSegmentedColormap

# Create a contingency table between 'Traveller_type' and 'Sentiment'
contingency_table = pd.crosstab(df_Traveller_type['Traveller_type'], df_Traveller_type['Sentiment'])

# Normalize the contingency table for better comparison
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Create a custom colormap with red and green colors
colors = [(1, 0, 0), (1, 1, 1), (0, 1, 0)]  # Red, White, Green
cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)

# Create a heatmap for the normalized contingency table with the custom colormap
plt.figure(figsize=(8, 6))
sns.heatmap(normalized_table, annot=True, cmap=cmap, fmt='.2f')
plt.title('Relation between Traveller Type and Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Traveller Type')
plt.show()

# Create a contingency table between 'Traveller_type' and 'Sentiment'
contingency_table = pd.crosstab(df_Traveller_type['Traveller_type'], df_Traveller_type['Sentiment'])

# Normalize the contingency table for better comparison
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Create a heatmap for the normalized contingency table
plt.figure(figsize=(8, 6))
sns.heatmap(normalized_table, annot=True, cmap='icefire', fmt='.2f')
plt.title('Relation between Traveller Type and Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Traveller Type')
plt.show()

# Create a contingency table between 'Traveller_type' and 'Sentiment'
contingency_table = pd.crosstab(df_Traveller_type['Traveller_type'], df_Traveller_type['Sentiment'])

# Plotting a heatmap for the contingency table (raw counts)
plt.figure(figsize=(8, 6))
sns.heatmap(contingency_table, annot=True, cmap='flare', fmt='d')
plt.title('Contingency Table: Traveller Type vs Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Traveller Type')
plt.show()

from matplotlib.colors import LinearSegmentedColormap

# Create a contingency table between 'Traveller_type' and 'Sentiment'
contingency_table = pd.crosstab(df_Traveller_type['Traveller_type'], df_Traveller_type['Sentiment'])

# Create a custom colormap
colors = [(1, 0, 0), (1, 1, 1), (0, 1, 0)]  # Red, White, Green
cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)

# Plotting a heatmap for the contingency table with reversed colors
plt.figure(figsize=(8, 6))
sns.heatmap(contingency_table, annot=True, cmap=cmap, fmt='d')
plt.title('Contingency Table: Traveller Type vs Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Traveller Type')
plt.show()

df_Traveller_type.info()

"""# **Creating new dataframe name df_fmtt for doing analysis based on Traveller_type & Flying_month column values those are available**"""

# Create a copy of the DataFrame df_fmtt without null values in 'Flying_month' or 'Traveller_type'
df_fmtt = df.dropna(subset=['Flying_month', 'Traveller_type']).copy()

df_fmtt.info()

"""# **• Research Question 4: Is there a relationship between flying month and traveller type?**"""

df_fmtt.to_csv(r'new31.csv')

# Convert 'Flying_month' column to datetime format if it's not already in datetime
df_fmtt['Flying_month'] = pd.to_datetime(df_fmtt['Flying_month'])

# Convert 'Flying_month' to the desired format 'Month YYYY' (e.g., 'June 2023')
df_fmtt['Formatted_month'] = df_fmtt['Flying_month'].dt.strftime('%B %Y')

# Display the DataFrame with the updated 'Formatted_month' column
print(df_fmtt[['Flying_month', 'Formatted_month']])

# Drop the original 'Flying_month' column
df_fmtt.drop(columns='Flying_month', inplace=True)

# Rename 'Formatted_month' column to 'Flying_month'
df_fmtt.rename(columns={'Formatted_month': 'Flying_month'}, inplace=True)

df_fmtt

df_fmtt['Flying_month'] = pd.to_datetime(df_fmtt['Flying_month'])

# Now, extract year and month from the 'Flying_month' column
df_fmtt['year'] = df_fmtt['Flying_month'].dt.year
df_fmtt['month'] = df_fmtt['Flying_month'].dt.month

df_fmtt.info()

df_fmtt

# Create a contingency table between 'Flying_month_categories' and 'Traveller_type'
contingency_table = pd.crosstab(df_fmtt['Flying_month'], df_fmtt['Traveller_type'])

# Perform the Chi-square test of independence
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Print the test statistics and p-value
print(f"Chi-square test statistic: {chi2}")
print(f"P-value: {p}")
print(f"Degrees of freedom: {dof}")

# Create a contingency table
contingency_table = pd.crosstab(df_fmtt['Flying_month'], df_fmtt['Traveller_type'])

# Perform chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# Interpret the results
alpha = 0.05
if p_val < alpha:
    print("Reject the null hypothesis. There is a significant relationship between Flying_month and Traveller_type.")
else:
    print("Fail to reject the null hypothesis. There is no significant relationship between Flying_month and Traveller_type.")

# Create a contingency table using crosstab
contingency_table = pd.crosstab(df_fmtt['year'], df_fmtt['Traveller_type'])

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap='coolwarm', linewidths=0.5)
plt.title('Contingency Table: Flying year vs Traveller Type')
plt.xlabel('Traveller Type')
plt.ylabel('Flying year')

plt.tight_layout()
plt.show()

# Grouping by month and traveler type and counting occurrences
monthly_traveller_counts = df_fmtt.groupby(['month', 'Traveller_type']).size().unstack(fill_value=0)

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(monthly_traveller_counts, annot=True, fmt='d', cmap='coolwarm', linewidths=0.5)
plt.title('Contingency Table: Flying Month vs Traveller Type')
plt.xlabel('Traveller Type')
plt.ylabel('Flying Month')

plt.tight_layout()
plt.show()

df_fmtt

# Convert 'Flying_month' to categories (e.g., months)
df_fmtt['Flying_month_categories'] = df_fmtt['Flying_month'].dt.strftime('%B')  # Month names from datetime

# Plotting a categorical scatter plot
plt.figure(figsize=(10, 6))
sns.stripplot(x='Flying_month_categories', y='Traveller_type', data=df_fmtt, jitter=True)
plt.title('Traveller Type Distribution Across Months')
plt.xlabel('Flying Month')
plt.ylabel('Traveller Type')
plt.xticks(rotation=45)
plt.show()

# Plotting a categorical scatter plot
plt.figure(figsize=(10, 6))
sns.stripplot(x='year', y='Traveller_type', data=df_fmtt, jitter=True)
plt.title('Traveller Type Distribution Across Years')
plt.xlabel('Flying Year')
plt.ylabel('Traveller Type')
plt.xticks(rotation=45)
plt.show()

# Boxplots can show the distribution of data across different years for each traveler type, displaying quartiles, outliers, and
# median values.
plt.figure(figsize=(10, 6))
sns.boxplot(x='month', y='Traveller_type', data=df_fmtt)
plt.title('Traveller Type Distribution Across Months')
plt.xlabel('Flying Month')
plt.ylabel('Traveller Type')
plt.xticks(rotation=0)
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='year', y='Traveller_type', data=df_fmtt)
plt.title('Traveller Type Distribution Across Years')
plt.xlabel('Flying Year')
plt.ylabel('Traveller Type')
plt.xticks(rotation=45)
plt.show()

# Violin plots combine the aspects of boxplot and kernel density estimation, providing insights into the distribution and
#probability density
plt.figure(figsize=(10, 6))
sns.violinplot(x='month', y='Traveller_type', data=df_fmtt)
plt.title('Traveller Type Distribution Across Months')
plt.xlabel('Flying Month')
plt.ylabel('Traveller Type')
plt.xticks(rotation=0)
plt.show()

plt.figure(figsize=(10, 6))
sns.violinplot(x='year', y='Traveller_type', data=df_fmtt)
plt.title('Traveller Type Distribution Across Years')
plt.xlabel('Flying Year')
plt.ylabel('Traveller Type')
plt.xticks(rotation=45)
plt.show()

# Plotting a countplot to show the distribution of 'Traveller_type' across months
plt.figure(figsize=(18, 8))
ax = sns.countplot(x='Flying_month_categories', hue='Traveller_type', data=df_fmtt)
plt.title('Traveller Type Distribution Across Months')
plt.xlabel('Flying Month')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Traveller Type')

# Annotate each bar with values and month names
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)

# Set custom labels for x-axis (month names)
months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
plt.xticks(ticks=range(0, len(months)), labels=months)

plt.tight_layout()
plt.show()

# Plotting a countplot to show the distribution of 'Traveller_type' across years
plt.figure(figsize=(18, 8))
ax = sns.countplot(x='year', hue='Traveller_type', data=df_fmtt)
plt.title('Traveller Type Distribution Across Years')
plt.xlabel('Flying Year')
plt.ylabel('Count')
plt.legend(title='Traveller Type')

# Annotate each bar with values
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)

plt.tight_layout()
plt.show()

# Create a DataFrame for percentage distribution of 'Traveller_type' across months
df_fmtt_month_percentage = df_fmtt.groupby(['month', 'Traveller_type']).size() / df_fmtt.groupby('month').size()
df_fmtt_month_percentage = df_fmtt_month_percentage.reset_index(name='Percentage')

# Plotting the percentage distribution of 'Traveller_type' across months
plt.figure(figsize=(12, 6))
sns.barplot(x='month', y='Percentage', hue='Traveller_type', data=df_fmtt_month_percentage, palette='viridis')
plt.title('Percentage Distribution of Traveller Type Across Months')
plt.xlabel('Flying Month')
plt.ylabel('Percentage')
plt.legend(title='Traveller Type')

# Formatting x-axis labels to display month names (assuming the month is in numeric format)
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
plt.xticks(ticks=range(12), labels=months)

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
barplot = sns.barplot(x='month', y='Percentage', hue='Traveller_type', data=df_fmtt_month_percentage, palette='viridis')

plt.title('Percentage Distribution of Traveller Type Across Months')
plt.xlabel('Flying Month')
plt.ylabel('Percentage')
plt.legend(title='Traveller Type')

# Add labels displaying the percentage values on top of each bar
for p in barplot.patches:
    barplot.annotate(format(p.get_height(), '.2f') + '%',
                     (p.get_x() + p.get_width() / 2., p.get_height()),
                     ha='center', va='bottom',
                     xytext=(0, 5),
                     textcoords='offset points',
                     fontsize=8)

# Formatting x-axis labels to display month names (assuming the month is in numeric format)
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
plt.xticks(ticks=range(12), labels=months)

plt.tight_layout()
plt.show()

# Create a DataFrame for percentage distribution of 'Traveller_type' across years
df_fmtt_year_percentage = df_fmtt.groupby(['year', 'Traveller_type']).size() / df_fmtt.groupby('year').size()
df_fmtt_year_percentage = df_fmtt_year_percentage.reset_index(name='Percentage')

# Plotting the percentage distribution of 'Traveller_type' across years with labels
plt.figure(figsize=(12, 6))
barplot = sns.barplot(x='year', y='Percentage', hue='Traveller_type', data=df_fmtt_year_percentage, palette='viridis')
plt.title('Percentage Distribution of Traveller Type Across Years')
plt.xlabel('Flying Year')
plt.ylabel('Percentage')
plt.legend(title='Traveller Type')

# Add labels displaying the percentage values on top of each bar
for p in barplot.patches:
    barplot.annotate(format(p.get_height(), '.2f') + '%',
                     (p.get_x() + p.get_width() / 2., p.get_height()),
                     ha='center', va='center',
                     xytext=(0, 5),
                     textcoords='offset points',
                     fontsize=8)

plt.tight_layout()
plt.show()

# Create a pivot table to count occurrences of combinations of 'Flying_month' and 'Traveller_type'
pivot_table = df_fmtt.pivot_table(index='month', columns='Traveller_type', aggfunc='size')

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(pivot_table, annot=True, fmt='d', cmap='coolwarm', linewidths=0.5)
plt.title('Frequency of Traveller Types per Flying Month')
plt.xlabel('Traveller Type')
plt.ylabel('Flying Month')

plt.tight_layout()
plt.show()

# Create a pivot table to count occurrences of combinations of 'Flying_month' and 'Traveller_type'
pivot_table = df_fmtt.pivot_table(index='month', columns='Traveller_type', aggfunc='size')

# Calculate the percentage distribution across 'Traveller_type' for each 'month'
percentage_table = pivot_table.div(pivot_table.sum(axis=1), axis=0) * 100

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(percentage_table, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Percentage Distribution of Traveller Types per Flying Month')
plt.xlabel('Traveller Type')
plt.ylabel('Flying Month')

plt.tight_layout()
plt.show()

# Create a pivot table to count occurrences of combinations of year' and 'Traveller_type'
pivot_table = df_fmtt.pivot_table(index='year', columns='Traveller_type', aggfunc='size')

# Calculate the percentage distribution across 'Traveller_type' for each 'month'
percentage_table = pivot_table.div(pivot_table.sum(axis=1), axis=0) * 100

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(percentage_table, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Percentage Distribution of Traveller Types per Flying Month')
plt.xlabel('Traveller Type')
plt.ylabel('year')

plt.tight_layout()
plt.show()

"""# **Creating new dataframe name df5 for doing analysis based on Traveller_type & Class column values those are available**"""

# Create a copy of the DataFrame df_fmtt without null values in 'Flying_month' or 'Traveller_type'
df5 = df.dropna(subset=['Class', 'Traveller_type']).copy()

df5.info()

"""# **Research question 5: How does the booking frequency differ across various classes (e.g., economy, business, first class) for different traveler types (e.g., Solo, Couple, & family leisure )?**"""

# Setting the order of 'Class' and 'Traveller_type' columns
class_order = ['Economy Class', 'Premium Economy Class', 'Business Class', 'First Class']
traveller_type_order = ['Solo Leisure', 'Couple Leisure', 'Family Leisure']

df5['Class'] = pd.Categorical(df5['Class'], categories=class_order, ordered=True)
df5['Traveller_type'] = pd.Categorical(df5['Traveller_type'], categories=traveller_type_order, ordered=True)

# Create a contingency table
contingency_table = pd.crosstab(df5['Class'], df5['Traveller_type'])

# Perform chi-squared test
chi2, p_val, dof, expected = chi2_contingency(contingency_table)

# Print chi-squared test result
print("Chi-squared statistic:", chi2)
print("p-value:", p_val)
print("Degree of Freedom:", dof)

# Interpretation of results
alpha = 0.05  # Set your chosen significance level here
if p_val < alpha:
    print('Reject null hypothesis: There is a significant relationship between Class and Traveller_type.')
else:
    print('Fail to reject null hypothesis: There is no significant relationship between Class and Traveller_type.')

g_stat, p_value, dof, expected = chi2_contingency(contingency_table, lambda_="log-likelihood")
print("G-test statistic:", g_stat)
print("p-value:", p_value)
print("Degree of Freedom:", dof)

# Plotting the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(contingency_table, annot=True, cmap="YlGnBu", fmt='d')
plt.title('Contingency Table: Class vs. Traveller Type')
plt.xlabel('Traveler Type')
plt.ylabel('Class')
plt.show()

# Set 'Class' and 'Traveller_type' columns as categorical with the specified order
df5['Class'] = pd.Categorical(df5['Class'], categories=class_order, ordered=True)
df5['Traveller_type'] = pd.Categorical(df5['Traveller_type'], categories=traveller_type_order, ordered=True)

# Create a contingency table
contingency_table = pd.crosstab(df5['Class'], df5['Traveller_type'], normalize='index') * 100  # Normalize to get percentages

# Plotting the heatmap with percentages
plt.figure(figsize=(8, 6))
ax = sns.heatmap(contingency_table, annot=True, cmap="icefire", fmt='.2f')  # Format as percentages

plt.title('Contingency Table(%): Class vs. Traveller Type')
plt.xlabel('Traveller Type')
plt.ylabel('Class')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df5, x='Class', hue='Traveller_type')

# Adding values on each bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.title('Counts of Classes by Traveler Types')
plt.xlabel('Class')
plt.ylabel('Count')
plt.legend(title='Traveler Type')
plt.show()

# Creating countplot for Class by Traveller_type
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df5, x='Class', hue='Traveller_type')

# Calculate percentages and add annotations to bars
total = df5['Class'].count()  # Total count of entries
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height / total * 100:.1f}%',
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.title('Percentage of Classes by Traveller Types')
plt.xlabel('Class')
plt.ylabel('Percentage')
plt.legend(title='Traveller Type')
plt.show()

"""# **Identify the factors affecting customer choices.**"""

# Creating a copy of the DataFrame 'df'
df6 = df.copy()

df6.info()

df.to_csv(r'new32.csv')

"""# I can see whether the class has any connections with review content it has which type of feedback by doing sentiment analysis and then we can see how the rating is given by the customer."""

# Initializing the SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Performing sentiment analysis
df6['Sentiment_Score'] = df6['Review_content'].apply(lambda x: sia.polarity_scores(str(x))['compound'])

# Define a threshold to categorize sentiment
threshold = 0.1

# Map sentiment scores to sentiment labels
df6['Sentiment'] = df6['Sentiment_Score'].apply(lambda score: 'positive' if score >= threshold else ('negative' if score <= -threshold else 'neutral'))

df6.info()

"""# There are many factors that could affect the customer choices such as:
1. Rating
2. reviews by doing sentiment analysis
3. Type of flight
4. Route
5. Class
6. Recommendation
"""

df6

df6['Sentiment'].value_counts()

# Check the distribution of Sentiment across different classes
plt.figure(figsize=(8, 6))
sns.countplot(x='Class', hue='Sentiment', data=df6)
plt.title('Sentiment Distribution across Classes')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

"""# Evaluate the influence of holiday booking time on customer behaviour."""

# Creating a copy of the DataFrame 'df'
df7 = df.copy()

# Drop rows with null values in the 'Flying_month' column
df7.dropna(subset=['Flying_month'], inplace=True)

# Convert 'Flying_month' column to datetime format
df7['Flying_month'] = pd.to_datetime(df7['Flying_month'], format='%b %Y')

# Extract month and year into new columns 'year' and 'month'
df7['month'] = df7['Flying_month'].dt.month_name()
df7['year'] = df7['Flying_month'].dt.year

df7.info()

"""# we can check the relationship between flying month and sentiment
The customer behaviour could be based on many factors such as:
1. Type of travel
2. Route
3. Class
4. Type of Flight
5. Rating
"""

# Initializing the SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Performing sentiment analysis
df7['Sentiment_Score'] = df7['Review_content'].apply(lambda x: sia.polarity_scores(str(x))['compound'])

# Define a threshold to categorize sentiment
threshold = 0.1

# Map sentiment scores to sentiment labels
df7['Sentiment'] = df7['Sentiment_Score'].apply(lambda score: 'Positive' if score >= threshold else ('Negative' if score <= -threshold else 'Neutral'))

df7

# Define custom colors for each sentiment category
custom_colors = {'Positive': 'lightcoral', 'Negative': 'lightgreen', 'Neutral': 'lightblue'}

# Sentiment Distribution across Flying Months (Bar plot by Month)
plt.figure(figsize=(12, 6))
sns.countplot(x='month', hue='Sentiment', data=df7, order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], palette=custom_colors.values()) # Change palette for different colors
plt.title('Sentiment Distribution across Flying Months')
plt.xlabel('Flying Month')
plt.ylabel('Count')

# Annotate count values on each bar for month plot with different style
for p in plt.gca().patches:
    plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                       ha='center', va='center', fontsize=9, color='black', xytext=(0, 5),
                       textcoords='offset points')

plt.show()

# Sentiment Distribution across Flying Years (Bar plot by Year)
plt.figure(figsize=(10, 6))
sns.countplot(x='year', hue='Sentiment', data=df7, palette=custom_colors.values()) # Change palette for different colors
plt.title('Sentiment Distribution across Flying Years')
plt.xlabel('Flying Year')
plt.ylabel('Count')

# Annotate count values on each bar for year plot with different style
for p in plt.gca().patches:
    plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                       ha='center', va='center', fontsize=9, color='black', xytext=(0, 5),
                       textcoords='offset points')

plt.show()

import pandas as pd
from scipy.stats import chi2_contingency


# Create a contingency table
contingency_table = pd.crosstab(df7['month'], df7['Sentiment'])

# Display the contingency table
print("Contingency Table:")
print(contingency_table)

# Perform Chi-Square test
chi2, p_value, _, _ = chi2_contingency(contingency_table)

# Display the test result
print("\nChi-Square Test Result:")
print("Chi2 Statistic:", chi2)
print("P-value:", p_value)

# Set the significance level
alpha = 0.05

# Interpret the results
if p_value < alpha:
    print("\nReject the null hypothesis. There is a significant relationship between Flying Month and Sentiment.")
else:
    print("\nFail to reject the null hypothesis. No significant relationship between Flying Month and Sentiment.")

import pandas as pd
from scipy.stats import chi2_contingency



# Create a contingency table
contingency_table = pd.crosstab(df7['month'], df7['Class'])

# Display the contingency table
print("Contingency Table:")
print(contingency_table)

# Perform Chi-Square test
chi2, p_value, _, _ = chi2_contingency(contingency_table)

# Display the test result
print("\nChi-Square Test Result:")
print("Chi2 Statistic:", chi2)
print("P-value:", p_value)

# Set the significance level
alpha = 0.05

# Interpret the results
if p_value < alpha:
    print("\nReject the null hypothesis. There is a significant relationship between Flying Month and Class.")
else:
    print("\nFail to reject the null hypothesis. No significant relationship between Flying Month and Class.")

"""# Predict the likelihood of a successful holiday booking based on customer characteristics."""

# Creating a copy of the DataFrame 'df'
df8 = df.copy()

"""the characteristics of customer is based on many factors such as:
1. Recommendation
2. Verification
3. reviews for sentiment analysis
"""

# Initializing the SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Performing sentiment analysis
df8['Sentiment_Score'] = df8['Review_content'].apply(lambda x: sia.polarity_scores(str(x))['compound'])

# Define a threshold to categorize sentiment
threshold = 0.1

# Map sentiment scores to sentiment labels
df8['Sentiment'] = df8['Sentiment_Score'].apply(lambda score: 'Positive' if score >= threshold else ('Negative' if score <= -threshold else 'Neutral'))

df8

"""# Recommended vs Sentiment"""

# Filter rows with non-null values in 'Sentiment' and 'Recommended' columns
df_recommended = df8.dropna(subset=['Sentiment', 'Recommended'])

# Set the order of categories for 'Recommended' and 'Sentiment'
recommended_order = ['Yes', 'No']
sentiment_order = ['Positive', 'Negative', 'Neutral']

# Countplot of Sentiment against Recommended with specified order
plt.figure(figsize=(8, 6))
sns.countplot(x='Sentiment', hue='Recommended', data=df_recommended, order=sentiment_order, hue_order=recommended_order, palette='Pastel2')
plt.title('Sentiment Distribution by Recommendation')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Annotate count values on each bar for better clarity
for p in plt.gca().patches:
    plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                       ha='center', va='center', fontsize=9, color='black', xytext=(0, 5),
                       textcoords='offset points')

plt.show()

from sklearn.preprocessing import LabelEncoder

# Label encode categorical columns 'Sentiment' and 'Recommended'
label_encoder = LabelEncoder()
df_recommended['Sentiment_encoded'] = label_encoder.fit_transform(df_recommended['Sentiment'])
df_recommended['Recommended_encoded'] = label_encoder.fit_transform(df_recommended['Recommended'])

# Create a correlation matrix between the encoded columns
correlation_matrix = df_recommended[['Sentiment_encoded', 'Recommended_encoded']].corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(6, 4))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation between Sentiment and Recommended')
plt.xlabel('')
plt.ylabel('')
plt.show()

"""# Verified vs Sentiment"""

# Filter rows with non-null values in 'Sentiment' and 'Verified' columns
df_verified = df8.dropna(subset=['Sentiment', 'Verified'])

# Set the order of categories for 'Sentiment'
sentiment_order = ['Positive', 'Negative', 'Neutral']

# Countplot of Sentiment against Verified for filtered data
plt.figure(figsize=(8, 6))
sns.countplot(x='Sentiment', hue='Verified', data=df_verified, order=sentiment_order, palette='Paired')
plt.title('Sentiment Distribution by Verification')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Annotate count values on each bar for better clarity
for p in plt.gca().patches:
    plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                       ha='center', va='center', fontsize=9, color='black', xytext=(0, 5),
                       textcoords='offset points')

plt.show()

# Filter rows with non-null values in 'Sentiment' and 'Verified' columns
df_verified1 = df8.dropna(subset=['Rating', 'Verified'])

# Label encode categorical columns 'Sentiment' and 'Recommended'
label_encoder = LabelEncoder()
df_verified['Sentiment_encoded'] = label_encoder.fit_transform(df_verified['Sentiment'])
df_verified['Recommended_encoded'] = label_encoder.fit_transform(df_verified['Recommended'])

# Create a correlation matrix between the encoded columns
correlation_matrix = df_verified[['Sentiment_encoded', 'Recommended_encoded']].corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(6, 4))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation between Sentiment and Recommended')
plt.xlabel('')
plt.ylabel('')
plt.show()

import pandas as pd
from scipy.stats import chi2_contingency

df9 = df8.dropna(subset=['Sentiment', 'Verified'])

# Create a contingency table
contingency_table = pd.crosstab(df9['Verified'], df9['Sentiment'])

# Display the contingency table
print("Contingency Table:")
print(contingency_table)

# Perform Chi-Square test
chi2, p_value, _, _ = chi2_contingency(contingency_table)

# Display the test result
print("\nChi-Square Test Result:")
print("Chi2 Statistic:", chi2)
print("P-value:", p_value)

# Set the significance level
alpha = 0.05

# Interpret the results
if p_value < alpha:
    print("\nReject the null hypothesis. There is a significant relationship between Verified and Sentiment.")
else:
    print("\nFail to reject the null hypothesis. No significant relationship between Verified and Sentiment.")

"""# Investigate the popularity of various routes and flight schedules"""

# Convert 'Flying_month' column to datetime format
df['Flying_month'] = pd.to_datetime(df['Flying_month'], format='%b %Y')

# Extract month and year into new columns 'year' and 'month'
df['month'] = df['Flying_month'].dt.month_name()
df['year'] = df['Flying_month'].dt.year

df.info()

df.to_csv(r'C:\Users\RRK\Desktop\Northumbria\ML Assesment\new23.csv')

# Count occurrences of each route
route_counts = df['Route'].value_counts()

top_routes = route_counts.head(25)
print("Top 10 Popular Routes:")
print(top_routes)

# Plotting the top routes with values on each bar
plt.figure(figsize=(10, 6))
ax = top_routes.plot(kind='bar', color='blue')
plt.title('Top 25 Popular Routes')
plt.xlabel('Routes')
plt.ylabel('Frequency')
plt.xticks(rotation=90)

# Annotate each bar with its value
for index, value in enumerate(top_routes):
    plt.text(index, value + 0.1, str(value), ha='center', va='bottom', fontsize=8)

plt.show()

df.info()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Filter the DataFrame for the top 25 routes
df_top_routes = df[df['Route'].isin(top_routes)]

# Create a cross-tabulation of Route and Month
route_month_cross_tab = pd.crosstab(df_top_routes['Route'], df_top_routes['month'])

# Plot a heatmap for the frequency table
plt.figure(figsize=(12, 8))
sns.heatmap(route_month_cross_tab, cmap="YlGnBu", annot=True, fmt='d', linewidths=.5)
plt.title('Frequency Analysis of Top 25 Popular Routes for Each Month')
plt.xlabel('Month')
plt.ylabel('Route')
plt.show()

"""# df9 is the new dataframe to do analysis based on the values present in flying month and route columns and not the whole dataframe df."""

# Creating a copy of the DataFrame 'df'
df9 = df.copy()

# Drop rows with null values in the 'Flying_month' column
df9.dropna(subset=['Flying_month', 'Route'], inplace=True)

# Convert 'Flying_month' column to datetime format
df9['Flying_month'] = pd.to_datetime(df9['Flying_month'], format='%b %Y')

# Extract month and year into new columns 'year' and 'month'
df9['month'] = df9['Flying_month'].dt.month_name()
df9['year'] = df9['Flying_month'].dt.year

# Count occurrences of each route
route_counts = df9['Route'].value_counts()

top_routes = route_counts.head(25)
print("Top 10 Popular Routes:")
print(top_routes)

# Plotting the top routes with values on each bar
plt.figure(figsize=(10, 6))
ax = top_routes.plot(kind='bar', color='blue')
plt.title('Top 10 Popular Routes')
plt.xlabel('Routes')
plt.ylabel('Frequency')
plt.xticks(rotation=90)

# Annotate each bar with its value
for index, value in enumerate(top_routes):
    plt.text(index, value + 0.1, str(value), ha='center', va='bottom', fontsize=8)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame 'df' with columns 'Route' and 'Flying_month'
# Replace this line with your actual DataFrame


top_routes = df9['Route'].value_counts().nlargest(25).index

# Filter the DataFrame for the top 25 routes
df_top_routes = df9[df9['Route'].isin(top_routes)]

# Create a cross-tabulation of Route and Month
route_month_cross_tab = pd.crosstab(df_top_routes['Route'], df_top_routes['Month'])

# Plot a heatmap for better visualization
plt.figure(figsize=(12, 8))
sns.heatmap(route_month_cross_tab, cmap="YlGnBu", annot=True, fmt='d', linewidths=.5)
plt.title('Frequency Analysis of Top 25 Popular Routes and Flying Months')
plt.xlabel('Month')
plt.ylabel('Route')
plt.show()

df9.info()

# Count occurrences of each route
route_counts = df9['Route'].value_counts()

# Display the top 10 most popular routes (you can adjust the number as needed)
top_routes = route_counts.head(10)
print("Top 10 Popular Routes:")
print(top_routes)

# Plotting the top routes with values on each bar
plt.figure(figsize=(14, 6))
ax = top_routes.plot(kind='bar', color='lightblue')
plt.title('Top 10 Popular Routes')
plt.xlabel('Routes')
plt.ylabel('Frequency')
plt.xticks(rotation=90)

# Annotate each bar with its value
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()),
                ha='center', va='bottom', fontsize=8)

plt.show()

# Count occurrences of each type of flight schedule
flight_schedule_counts = df9['Type_of_flight'].value_counts()

# Display the flight schedule counts
print("Flight Schedule Counts:")
print(flight_schedule_counts)

# Plotting flight schedule counts
plt.figure(figsize=(14, 6))
flight_schedule_counts.plot(kind='bar', color='lightgreen')
plt.title('Flight Schedule Counts')
plt.xlabel('Type of Flight Schedule')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame 'df' with columns 'Route' and 'Flying_month'
# Replace this line with your actual DataFrame
# df = ...
df10 = df.sub
# Get the top 25 popular routes
top_routes = df['Route'].value_counts().nlargest(25).index

# Filter the DataFrame for the top 25 routes
df_top_routes = df[df['Route'].isin(top_routes)]

# Convert 'Flying_month' to datetime format if it's not already
df_top_routes['Flying_month'] = pd.to_datetime(df_top_routes['Flying_month'])

# Extract month from 'Flying_month'
df_top_routes['Month'] = df_top_routes['Flying_month'].dt.month_name()

# Create a cross-tabulation of Route and Month
route_month_cross_tab = pd.crosstab(df_top_routes['Route'], df_top_routes['Month'])

# Plot a heatmap for better visualization
plt.figure(figsize=(12, 8))
sns.heatmap(route_month_cross_tab, cmap="YlGnBu", annot=True, fmt='d', linewidths=.5)
plt.title('Frequency Analysis of Top 25 Popular Routes and Flying Months')
plt.xlabel('Month')
plt.ylabel('Route')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame 'df' with columns 'Route' and 'Flying_month'
# Replace this line with your actual DataFrame
# df = ...

# Get the top 25 popular routes
top_routes = df['Route'].value_counts().nlargest(25).index

# Filter the DataFrame for the top 25 routes
df_top_routes = df[df['Route'].isin(top_routes)]

# Convert 'Flying_month' to datetime format if it's not already
df_top_routes['Flying_month'] = pd.to_datetime(df_top_routes['Flying_month'])

# Extract month from 'Flying_month'
df_top_routes['Month'] = df_top_routes['Flying_month'].dt.month_name()

# Create a cross-tabulation of Route and Month
route_month_cross_tab = pd.crosstab(df_top_routes['Route'], df_top_routes['Month'],margins=True, margins_name='Total')

# Plot a heatmap for better visualization
plt.figure(figsize=(12, 8))
sns.heatmap(route_month_cross_tab, cmap="YlGnBu", annot=True, fmt='d', linewidths=.5)
plt.title('Frequency Analysis of Top 25 Popular Routes and Flying Months')
plt.xlabel('Month')
plt.ylabel('Route')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame 'df' with the column 'Flying_month'
# Replace this line with your actual DataFrame
# df = ...

# Convert 'Flying_month' to datetime format if it's not already
df['Flying_month'] = pd.to_datetime(df['Flying_month'])

# Extract month from 'Flying_month'
df['Month'] = df['Flying_month'].dt.month_name()

# Group by month and count the number of flights
flights_per_month = df.groupby('Month').size()

# Plot a bar chart
plt.figure(figsize=(10, 6))
bars = flights_per_month.plot(kind='bar', color='green', alpha=0.7)

# Add total on top of each bar
for bar in bars.patches:
    plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, bar.get_height() + 0.1, str(int(bar.get_height())), ha='center', color='black')

plt.title('Total Flights per Month')
plt.xlabel('Month')
plt.ylabel('Number of Flights')
plt.show()

df9